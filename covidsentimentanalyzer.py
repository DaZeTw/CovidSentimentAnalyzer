# -*- coding: utf-8 -*-
"""CovidSentimentAnalyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ipe0sIj-niigV3_-V7rj5iWGKxlDg-UT
"""

pip install plotly_express

pip install pyLDAvis

pip install wordcloud

pip install --upgrade pandas numpy

pip install --upgrade pandas

pip install --upgrade pandas seaborn

pip install --upgrade numpy

pip install --upgrade nltk scikit-learn plotly

"""# Exploring Public Sentiment on Twitter: An NLP Approach

## Part 1: Setting Up
"""

# Importing InteractiveShell from IPython.core.interactiveshell module
from IPython.core.interactiveshell import InteractiveShell

# Setting the ast_node_interactivity option of InteractiveShell to "all"
# This allows IPython to display results for all statements in a code cell
# rather than just the last one, which is the default behavior.
InteractiveShell.ast_node_interactivity = "all"

import warnings
import matplotlib

# Suppress the specific MatplotlibDeprecationWarning
warnings.filterwarnings("ignore", category=matplotlib.cbook.MatplotlibDeprecationWarning)

"""### Task 1: Import the Libraries"""

import os
import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
import plotly_express as px
from gensim.corpora import Dictionary
from gensim.models.ldamulticore import LdaMulticore
from gensim.models.coherencemodel import CoherenceModel
import pyLDAvis.gensim
from wordcloud import WordCloud
import nbconvert
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from google.colab import drive
drive.mount('/content/drive')

nltk.download('punkt')

nltk.download('stopwords')

nltk.download('wordnet')

nltk.download('omw-1.4')

nltk.download('vader_lexicon')

"""## Part 2: Data Collection and Preprocessing

### Task 2: Load the Dataset and Have a First Look

##### Load the CSV into a variable named `df_twitter`
"""

df_twitter = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/covid19_twitter_dataset.csv",index_col=0)

"""##### Display the DataFrame along with the row count"""

df_twitter.head()
df_twitter.shape

"""##### Show the tweet count for the top 10 countries"""

df_twitter['country'].value_counts()[:10]

"""##### Plot the top 20 users who post the most"""

df_twitter['user_name'].value_counts()[:20].plot(kind='barh')

"""### Task 3: Basic Text Preprocessing

##### Remove unnecessary columns for the analysis
"""

df_twitter.drop(['user_description','user_created','user_favourites','language'],axis=1,inplace=True)

"""##### Check for missing values and handle them"""

df_twitter.isnull().sum()

df_twitter['hashtags'].fillna("[]",inplace=True)

df_twitter.isnull().sum()

"""##### Convert `date` column to datetime object and extract features"""

df_twitter['date'] = pd.to_datetime(df_twitter['date'])
df_twitter['year'] = df_twitter['date'].dt.year
df_twitter['month'] = df_twitter['date'].dt.month
df_twitter['day'] = df_twitter['date'].dt.day
df_twitter['hour'] = df_twitter['date'].dt.hour
df_twitter['day_of_week'] = df_twitter['date'].dt.dayofweek

"""##### Apply `basic_clean_text()` function to `text` column"""

def basic_clean_text(text):
    text=text.lower()
    text = re.sub(r" +", ' ', text, flags=re.MULTILINE)
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = text.replace('<.*?','')
    text = text.replace('[^A-Za-z0-9 ]+', '')
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    return text

df_twitter['text'] = df_twitter['text'].apply(basic_clean_text)

"""### Task 4: Implement Advanced Text Preprocessing

##### Apply `advanced_text_preprocessing()` function to `text` column
"""

def advanced_text_preprocessing(text):
    tokens = word_tokenize(text)

    stop_words = set(stopwords.words('english'))
    filter_tokens = [word for word in tokens if word.lower() not in stop_words]

    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filter_tokens]

    preprocessed_text = " ".join(lemmatized_tokens)

    return preprocessed_text

df_twitter['text'] = df_twitter['text'].apply(advanced_text_preprocessing)

"""## Part 3: Sentiment Analysis

### Task 5: Perform Sentiment Analysis with Vader Library

##### Perform sentiment analysis with vader library
"""

sid = SentimentIntensityAnalyzer()

df_twitter["sentiment_scores"] = df_twitter['text'].apply(lambda x:sid.polarity_scores(x))

"""##### Display a random sample of 10 tweets with their sentiment scores"""

df_twitter[['text','sentiment_scores']].sample(10).values

"""### Task 6: Classify the Tweets into Positive, Neutral and Negative

##### Classify the tweets into categories of positive, negative, or neutral sentiment
"""

threshold_value = 0.0
df_twitter['sentiment'] = df_twitter['sentiment_scores'].apply(lambda x:'positive'
 if x['compound']>threshold_value else ('neutral' if x['compound'] == threshold_value else 'negative'))

print (df_twitter['sentiment'].value_counts())

"""## Part 4: Trend Analysis and Visualization

### Task 7: Display the Evolution of Sentiment Over Time

##### Extract the top 3 countries to be used as a filter
"""

most_active_countries = df_twitter['country'].value_counts().nlargest(3).index.tolist()
most_active_countries

"""##### Filter the DataFrame"""

filtered_data = df_twitter[df_twitter['country'].isin(most_active_countries)]
filtered_data.shape

"""##### Create sentiment over time by country based on `groupby`"""

sentiment_over_time_by_country = filtered_data.groupby([pd.Grouper(key='date', freq='D'), 'country'])['sentiment'].value_counts().unstack().fillna(0).reset_index()

# Now melt the DataFrame
sentiment_melted = sentiment_over_time_by_country.melt(id_vars=['date', 'country'], value_vars=['negative', 'neutral', 'positive'], var_name='sentiment', value_name='count')

"""##### Plot the top three countries sentiments over time"""

for country in most_active_countries:
    _ = plt.figure(figsize=(15,6))
    _ = sns.lineplot(data=sentiment_melted[sentiment_melted['country']==country],x='date',y='count',hue='sentiment')
    _ = plt.title(f'Sentiment Counts Over Time for {country}')
    _ = plt.show()

"""### Task 8: Use Wordcloud to Visualize Words Used in Sentiments

##### Define the `create_word_cloud()` function
"""

def create_word_cloud(sentiment):
    text=" ".join(df_twitter[df_twitter['sentiment']==sentiment]['text'].values)

    wordcloud = WordCloud(width=800,height=400,background_color='white').generate(text)
    plt.figure(figsize=(15,6))
    plt.imshow(wordcloud,interpolation='bilinear')
    plt.axis("off")
    plt.title(f'Word Cloud for {sentiment.capitalize()} Sentiments')
    plt.show()

"""##### Creating word cloud for positive sentiment"""

create_word_cloud('positive')

"""##### Creating word cloud for negative sentiment"""

create_word_cloud('negative')

"""##### Creating word cloud for neutral sentiment"""

create_word_cloud('neutral')

"""### Task 9: Display the Sentiment on a Geographical Heatmap

##### Create mapping from positive, neutral and negative to an numerical value for visualization purpose
"""

sentiment_mapping = {'positive':1,'neutral':0,'negative':-1}
df_twitter['sentiment_value'] = df_twitter['sentiment'].map(sentiment_mapping)

"""##### Visualize a geographical heatmap of all sentiment on the map"""

fig = px.density_mapbox(df_twitter, lat='lat', lon='long',
                        z='sentiment_value', radius=20,
                        center=dict(lat=df_twitter.lat.mean(),
                                    lon=df_twitter.long.mean()),
                        zoom=4,
                        mapbox_style="open-street-map",
                        height=900)
fig.show()

"""## Part 5: Topic Modeling

### Task 10: Train the LDA (Gensim) Model

##### Preprocess the text
"""

df_twitter['text_tokens'] = df_twitter['text'].str.lower().str.split()

"""##### Create a dictionary"""

id2word = Dictionary(df_twitter['text_tokens'])

"""##### Filter extremes"""

id2word.filter_extremes(no_below=2, no_above=.99)

"""##### Create a corpus"""

corpus = [id2word.doc2bow(d) for d in df_twitter['text_tokens']]

"""##### Instantiate an LDA model"""

base_model = LdaMulticore(corpus=corpus, num_topics=5, id2word=id2word, workers=12,
passes=5)

"""### Task 11: Evaluate the Model

##### Print the topics
"""

words = [re.findall(r'"([^"]*)"',t[1]) for t in base_model.print_topics()]

topics = [" ".join(t[0:10]) for t in words]

for id,t in enumerate(topics):
    print(f"------ Topic {id} ------")
    print(t, end="\n\n")

"""##### Compute the perplexity and coherence score"""

base_perplexity = base_model.log_perplexity(corpus)
print('\nPerplexity: ', base_perplexity)

coherence_model = CoherenceModel(model=base_model, texts=df_twitter['text_tokens'],
                                   dictionary=id2word, coherence='c_v')
coherence_lda_model_base = coherence_model.get_coherence()
print('\nCoherence Score: ', coherence_lda_model_base)

"""### Task 12: Classify Twitter Tweets into Topics

##### Topic classification function
"""

def classify_tweet(tweet):
    processed_tweet = tweet.lower().split()
    tweet_bow = id2word.doc2bow(processed_tweet)
    topic_probabilities = base_model.get_document_topics(tweet_bow)
    most_likely_topic = max(topic_probabilities,key = lambda x:x[1])
    return most_likely_topic[0]

"""##### Classify all tweets"""

df_twitter['topic'] = df_twitter['text'].apply(lambda x: classify_tweet(x))

"""##### Examine topic distribution"""

df_twitter.topic.value_counts()

"""## Part 6: Interpretation of Results

### Task 13: Identify Relationships between Sentiment and Topic

##### Grouping and aggregating data by topic and sentiment
"""

grouped = df_twitter.groupby(['topic','sentiment']).size().unstack(level='sentiment')
print(grouped)

"""##### Calculating proportions"""

percent_grouped = grouped.divide(grouped.sum(axis=1),axis=0)
print(percent_grouped)

"""##### Visualizing the results"""

palette = {'positive': '#66BB6A', 'neutral': '#BDBDBD', 'negative': '#EF5350'}
colors = [palette[col] for col in percent_grouped.columns]

percent_grouped.plot(kind='bar', stacked=True, color=colors)
plt.xlabel('Topic')
plt.ylabel('Proportion of Tweets')
plt.title('Proportion of Sentiments by Topic')
plt.legend(loc='upper right')
plt.show()

"""### Task 14: Interpret the Topic Modeling Results

##### Creating topic distance visualization
"""

pyLDAvis.enable_notebook()
pyLDAvis.gensim.prepare(base_model,corpus,id2word)

"""### Task 15: Compile your Findings into a Final Report with NBConvert

##### Execute the command within the provided notebook cell
"""

!jupyter nbconvert --to html "C:\Users\drake\Desktop\Jupyter notebook\CovidSentimentAnalyzer.ipynb" --output-dir="C:\Users\drake\Desktop\Jupyter notebook"

"""# CONGRATULATIONS"""